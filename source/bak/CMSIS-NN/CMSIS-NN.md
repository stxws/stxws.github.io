---
title: 'CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs'
date: 2021-04-28
categories: 论文解读
tags: [arm, 嵌入式, 神经网络, 深度学习]
mathjax: true
---

[论文原文下载链接](https://cdn.jsdelivr.net/gh/stxws/stxws.github.io/documnets/papers/CMSIS-NN.pdf)

# 摘要

&emsp;&emsp;深度神经网络在始终在线的IoT外围设备中变得越来越流行，这些IoT外围设备从源头开始执行数据分析，从而减少了数据通信的延迟和能耗。本文介绍了CMSIS-NN，这些高效的内核经过开发，可在针对智能IoT外围设备的Arm Cortex-M处理器上最大化性能并最小化神经网络（NN）应用程序的内存占用量。基于CMSIS-NN内核的神经网络推理可将运行时间/吞吐量提高4.6倍，将能效提高4.9倍。


# 介绍

&emsp;&emsp;过去几年中，互联设备（也称为物联网（IoT））迅速增长，预计到2035年在各个市场领域将达到1万亿[1]。这些IoT外围设备通常由收集数据（例如音频，视频，温度，湿度，GPS位置和加速度）的传感器组成，然后进行处理并与其他节点或云进行通信。当前，来自传感器的数据由云中的分析工具进行处理，以实现广泛的应用，例如工业监控，家庭自动化和医疗保健。但是，随着IoT节点数量的增加，这给网络带宽带来了相当大的负担，并给IoT应用程序增加了延迟。此外，对云的依赖使在网络连接受限或不可靠的区域中部署物联网应用程序带来了挑战。解决此问题的一种解决方案是外围计算[2]，它直接在数据源即IoT外围节点上执行，从而减少了等待时间并节省了数据通信的功耗。

&emsp;&emsp;在准确性方面，深度神经网络已经证明了在许多复杂的机器学习应用程序中的有接近人的表现，例如图像分类，语音识别和自然语言处理。一个典型的用于图像分类的神经网络（NN）由多层基于卷积的特征提取器组成，然后是用于分类的全连接层，如图1所示。由于计算复杂性和资源需求，神经网络的执行主要限于使用高性能服务器CPU或专用硬件（例如GPU或加速器）进行的云计算，这会增加IoT应用程序的延迟。在数据源即IoT外围处使用小型神经网络进行分类可以减少IoT外设与云之间的数据通信的总体延迟和能耗。

&emsp;&emsp;在这项工作中，我们探索了基于资源受限的微控制器平台上神经网络的性能优化，该平台针对智能物联网外围节点。为此，我们开发了优化的软件内核，用于在Arm Cortex-M CPU上部署NN。使用这些软件内核，我们在现成的Arm Cortex-M7平台上演示了针对CIFAR-10数据集的卷积神经网络（CNN），该算法每秒可分类10.1张图像，准确度为79.9％。


# 概述

&emsp;&emsp;神经网络内核的概述如图2所示。内核代码由两部分组成：`NNFunctions`和`NNSupportFunctions`。`NNFunctions`包含了流行的神经网络层类型的函数实现，例如卷积，深度可分离卷积，全连接（即内积），池化和激活。应用程序代码可以使用这些函数来实现神经网络推理应用程序。内核API也保持简单，因此可以轻松地将它们移植到任何机器学习框架。`NNSupportFunctions`包括实用程序功能，例如`NNFunctions`中使用的数据转换和激活函数表。应用程序代码还可使用这些实用程序功能来构建更复杂的NN模块，例如长短期记忆（LSTM）或门控循环单元（GRU）。

&emsp;&emsp;对于某些内核，例如全连接和卷积，将实现不同版本的内核函数。提供了一个基本版本，可以对所有层参数普遍使用“原样”。我们还实现了其他版本，其中包括进一步的优化技术，这些技术要么具有转换后的输入，要么对层参数有一些限制。理想情况下，可以使用简单的脚本来解析网络拓扑并自动确定要使用的适当功能。


# 定点量化

&emsp;&emsp;传统上，使用32位浮点数据表示来训练NN模型。但是，推理期间通常不需要这种高精度。研究表明，即使使用低精度定点表示，神经网络也能很好地运行。定点量化有助于避免昂贵的浮点计算，并减少用于存储权重和激活的内存占用，这对于资源受限的平台至关重要。

&emsp;&emsp;尽管不同网络或网络层的精度要求可能会有所不同，但CPU很难对位宽不同的数据类型进行操作。在这项工作中，我们开发了支持8位和16位数据的内核。内核采用与CMSIS中相同的数据类型格式，即$q7\_t$作为$int8$，$q15\_t$作为$int16$，$q31\_t$作为$int32$。假定以2的幂进行定点格式量化，即表示的值将为$A \times 2^n$，其中$A$为整数值，$n$表示小数点位置的整数。我们将偏置的缩放因子传递给内核，并将其作为参数传递给内核，并且由于二次幂缩放，缩放可以用按位移位运算实现。我们使用这种类型的量化（而不是TensorFlow中使用的8位量化）来避免在层之间进行浮点反量化，因为某些Arm Cortex-M CPU可能没有专用的浮点单元（FPU），从而限制了它们的浮点计算能力。这种量化的另一个好处是我们可以使用更简单的基于表查找的激活，这将在第4.5节中讨论。


# 软件内核

&emsp;&emsp;在本节中，我们描述了提出的软件内核的实现和优化，用于Arm Cortex-M CPU。Cortex-M 系列处理器是32位RISC处理器内核，旨在提高能效，通常用作深度嵌入式应用的微控制器。在这项工作中，我们专注于在支持SIMD指令且基于Cortex-M的系统上启用神经网络，尤其是对NN计算非常有用的16位乘法和累加（MAC）指令（例如SMLAD）。

## Support Functions

&emsp;&emsp;大多数NNFunction使用16位MAC指令，因此需要数据转换才能将8位数据类型（即$q7\_t$）转换为16位数据类型（即$q15\_t$）。CMSIS提供了一个实用程序函数`arm_q7_to_q15`来执行数据转换。图示和伪代码如图3所示。数据转换分为两个步骤：第一步，使用符号扩展指令（__SXTB16）将8位数据扩展为16位数据。第二步重新排列数据，以便输出遵循与输入相同的顺序。

&emsp;&emsp;数据转换的性能是至关重要的，因为它被用于计算内核的内部循环。虽然符号扩展的第一步很重要，但是如果两个操作数遵循相同的顺序，则可以省略重新排列数据的第二步。为了更好地利用这一点，我们创建了另一个版本的数据转换例程，而无需对数据进行重新排序，如图4所示。该例程在4.2节中进行了详细讨论。

## 矩阵乘法

&emsp;&emsp;矩阵乘法是神经网络中最重要的计算内核。这项工作的实现基于CMSIS中的mat_mult内核。与CMSIS实现类似，矩阵乘法内核由2×2内核实现，如图5所示。这可以实现一些数据重用，并节省加载指令的总数。累加是通过q31_t数据类型完成的，两个操作数都是q15_t数据类型。我们用相应的偏置值初始化累加器。使用专用MAC指令__SMLAD执行计算。

## 卷积

&emsp;&emsp;卷积层通过计算滤波器权重与输入特征图中的小接受域之间的点积来提取新的特征图。通常，基于CPU的卷积实现分解为输入重新排序和扩展（即im2col，image-to-column）和矩阵乘法运算。im2col是将类似图像的输入转换为代表每个卷积滤波器所需数据的列的过程。im2col的示例如图8所示。

&emsp;&emsp;im2col的主要挑战之一是增加的内存占用空间，因为在im2col输出矩阵中重复了输入图像中的像素。为了减轻内存占用量的问题，同时保留im2col的性能优势，我们为卷积内核实现了部分im2col。内核将只扩展有限数量的列（例如2列），足以使矩阵乘法内核获得最大的性能提升，同时又使内存开销最小。

&emsp;&emsp;图像数据格式也会影响卷积性能，尤其是im2col效率。批处理大小为1时，卷积操作是对3D数据的2D卷积（即卷积窗口可以在两个方向上移动），如图9所示。两种最常见的图像数据格式是“通道-宽-高”（CHW），即通道在最后，和“高-宽-频道”（HWC），即频道第一。维度顺序与数据步长的顺序相同。在HWC格式中，沿通道的数据以步长为1存储，沿宽度的数据以通道数的步长存储，沿高度的数据以（通道数×图像宽度）步长存储。


